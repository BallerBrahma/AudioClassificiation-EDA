{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18b366b-104d-4751-b08b-2b28117c9417",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46cccf7-ae56-4037-8270-0481375f2886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a55665c-e3d9-468a-85f5-f77482add183",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'UrbanSound8k/bark.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807ce861-f4ef-491f-b850-88cf3160ad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import librosa \n",
    "import librosa.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe7f279-bdb9-40c8-b144-33c1c3a8f6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example: Graphing the sample rates of the audio\n",
    "#Librosa tries to converts the singal into a mono channel\n",
    "plt.figure(figsize=(14,5))\n",
    "data,sample_rate = librosa.load(filename)\n",
    "librosa.display.waveshow(data, sr=sample_rate)\n",
    "ipd.Audio(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bb384f-a15a-4a9e-982c-fec41e9096f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random file name\n",
    "filename = 'UrbanSound8k/104817-4-0-8.wav'\n",
    "plt.figure(figsize=(14,5))\n",
    "data,sample_rate = librosa.load(filename)\n",
    "librosa.display.waveshow(data, sr=sample_rate)\n",
    "ipd.Audio(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6cf0b6-a59d-4fcc-a779-d5712a164486",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d0560b-38e3-4af2-8ba4-70ece2b93b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import wavfile as wav\n",
    "wave_sample_rate, wave_audio = wav.read(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ea4f3d-8d49-4a23-a300-413c5fa0383d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b70faa9-691e-4f8b-88ef-565c9f17365e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each audio signal is represented by an integer value, combining the values creates the wave\n",
    "#Not normalized\n",
    "wave_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf571f24-b0b2-4ce1-9529-80ab57548f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librosa converts signals into normalized data\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d59866e-2523-45a5-ad6d-d3974d17a65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "metadata = pd.read_csv('UrbanSound8k/metadata/UrbanSound8k.csv')\n",
    "metadata.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eb6b65-3f4c-4271-8bdb-ee8d19ffe4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check whether the dataset is imbalanced\n",
    "metadata['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5567ac94-d3a8-4457-b8d6-85f6d80a0938",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's read a sample audio using librosa\n",
    "audio_file_path='UrbanSound8K/104817-4-0-8.wav'\n",
    "librosa_audio_data,librosa_sample_rate=librosa.load(audio_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2518d1b-a118-4f78-ba66-7651dbcc4395",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(librosa_audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9666c41d-ab33-4270-8fd1-04a03f843466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Original audio with 1 channel \n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(librosa_audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3271ed4-0652-4b76-a569-eed86d00f870",
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_sample_rate, wave_audio = wav.read(audio_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02223849-f7a6-4c17-bc5f-9890507f7a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7928a51b-d472-4be6-a4d6-f63295d914f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original audio with 2 channels \n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(wave_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffcf036-f187-41bb-af1f-a03c18bc18e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs = librosa.feature.mfcc(y=librosa_audio_data, sr=librosa_sample_rate, n_mfcc=40)\n",
    "print(mfccs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b669165b-3b57-4da5-b996-619504344a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc070b1f-720a-4ee0-8fa0-4cba6b14b4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Extracting MFCC's For every audio file\n",
    "import os\n",
    "\n",
    "audio_dataset_path='UrbanSound8K/audio/'\n",
    "metadata=pd.read_csv('UrbanSound8K/metadata/UrbanSound8K.csv')\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb8519e-b3fb-4231-ae19-d8c2d82b039b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_extractor(file):\n",
    "    audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "    mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "    mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)\n",
    "    \n",
    "    return mfccs_scaled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dd4eb2-6906-40d2-bdd7-1140eb86caf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "### Now we iterate through every audio file and extract features \n",
    "### using Mel-Frequency Cepstral Coefficients\n",
    "extracted_features=[]\n",
    "for index_num,row in tqdm(metadata.iterrows()):\n",
    "    file_name = os.path.join(os.path.abspath(audio_dataset_path),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n",
    "    final_class_labels=row[\"class\"]\n",
    "    data=features_extractor(file_name)\n",
    "    extracted_features.append([data,final_class_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d00006-d450-4587-87df-94d0bc0228f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### converting extracted_features to Pandas dataframe\n",
    "extracted_features_df=pd.DataFrame(extracted_features,columns=['feature','class'])\n",
    "extracted_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb6eee4-555f-4df4-bac3-439de0206fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split the dataset into independent and dependent dataset\n",
    "X=np.array(extracted_features_df['feature'].tolist())\n",
    "y=np.array(extracted_features_df['class'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ebb86c-7e50-44d4-9df1-b1e14a35831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Label Encoding\n",
    "y=np.array(pd.get_dummies(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f91683a-c593-49f8-90ea-d736ee73cfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e561f2-0ac8-4033-a82e-f492c4514c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b97fee-077e-4a1b-a195-94d7b65060a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cd03f5-ba86-43a5-a936-9a365a563193",
   "metadata": {},
   "outputs": [],
   "source": [
    "### No of classes\n",
    "num_labels=y.shape[1]\n",
    "Dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685e9bb6-2781-4241-9ad4-c44dcdf06b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "###first layer\n",
    "model.add(Dense(100,input_shape=(40,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "###second layer\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "###third layer\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "###final layer\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c1bf7f-8a9b-4a6e-b135-0bef1da9502f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c69d529-2fb3-486a-b173-241d782e592b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training my model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from datetime import datetime \n",
    "\n",
    "num_epochs = 100\n",
    "num_batch_size = 32\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/audio_classification.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(X_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c013aa-e905-423f-82ec-5f4ed63af98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy=model.evaluate(X_test,y_test,verbose=0)\n",
    "print(test_accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08ded23-eccf-41f3-8ca3-767794a8e7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"UrbanSound8K/dog_bark.wav\"\n",
    "prediction_feature=features_extractor(filename)\n",
    "prediction_feature=prediction_feature.reshape(1,-1)\n",
    "model.predict_classes(prediction_feature)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
